{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyOh/fsKhyMUtSlBP59K6q6M"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/EmanueleCeglia/PDF-Metadata-Extractor.git"
      ],
      "metadata": {
        "id": "MHIbcKwxLuKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd PDF-Metadata-Extractor"
      ],
      "metadata": {
        "id": "Zf98MyPQL2Lz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6470c5d-6c5c-474f-9931-77fa5fcd7845"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PDF-Metadata-Extractor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "U0w-7EwSL9kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install ghostscript -y"
      ],
      "metadata": {
        "id": "MNXmHnKO4x0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Dates"
      ],
      "metadata": {
        "id": "UiWcGaIttFL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from pdf_extractor import PDFDatesFinderSemanticSearch  #old method\n",
        "from pdf_extractor import PDFDatesFinderSpace   #new method\n",
        "from pdf_extractor import PDFDeductiblesFinder\n",
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import json"
      ],
      "metadata": {
        "id": "Sl0zsSAz7gHE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download model"
      ],
      "metadata": {
        "id": "2zaVl7xYvVfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Open-Orca/OpenOrca-Platypus2-13B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Open-Orca/OpenOrca-Platypus2-13B\", device_map=\"auto\", load_in_8bit=True)"
      ],
      "metadata": {
        "id": "i6qxZyr37hMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def post_process_response_dates(text):\n",
        "    start_date_pattern = r\"Start date:\\s*([0-9]+[a-z]*\\s+[A-Z][a-z]+\\s+[0-9]{4})\"\n",
        "    end_date_pattern = r\"End date:\\s*([0-9]+[a-z]*\\s+[A-Z][a-z]+\\s+[0-9]{4})\"\n",
        "\n",
        "    start_date_match = re.search(start_date_pattern, text)\n",
        "    end_date_match = re.search(end_date_pattern, text)\n",
        "\n",
        "    result = {}\n",
        "    if start_date_match and end_date_match:\n",
        "        result['Start date'] = start_date_match.group(1)\n",
        "        result['End date'] = end_date_match.group(1)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "    return result\n",
        "\n",
        "# second level filter\n",
        "def find_dates_regex(string):\n",
        "    pattern = r'\\b\\d{1,2}/\\d{1,2}/\\d{4}\\b|\\b\\d{1,2} [A-Za-z]+ \\d{4}\\b|\\b\\d{1,2}(?:st|nd|rd|th)? [A-Za-z]+ \\d{4}\\b|\\b[A-Za-z]+ \\d{1,2}(?:st|nd|rd|th)? \\d{4}\\b'\n",
        "    dates = re.findall(pattern, string)\n",
        "    if len(dates) >= 2:\n",
        "        return f\"Start date: {dates[0]} End date: {dates[1]}\"\n",
        "    elif len(dates) == 1:\n",
        "        return f\"Start date: {dates[0]} End date: None\"\n",
        "    else:\n",
        "        return \"No dates found\""
      ],
      "metadata": {
        "id": "ihmEWA-uB9Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dates extraction with OLD METHOD (don't use)"
      ],
      "metadata": {
        "id": "titiBIbjBvUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "insurances_folder_path = \"/content/PDF-Metadata-Extractor/Insurances\"\n",
        "\n",
        "extracted_dates = {}\n",
        "\n",
        "for root, dirs, files in os.walk(insurances_folder_path):\n",
        "    for file_name in tqdm(files):\n",
        "        full_file_path = os.path.join(root, file_name)\n",
        "\n",
        "        extraction = PDFDatesFinderSemanticSearch(full_file_path)\n",
        "        extraction.load_pdf()\n",
        "        extraction.process_text()\n",
        "        dates = extraction.find_dates()\n",
        "\n",
        "        for value in dates.values():\n",
        "          for phrase in value.values():\n",
        "            if len(phrase):\n",
        "              sentence = phrase[0]\n",
        "\n",
        "              prompt = \"Find start date and end date from the following sentence: \" + sentence\n",
        "              inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "              generate_ids = model.generate(inputs.input_ids, max_length=1000)\n",
        "              response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "              response = post_process_response_dates(response)\n",
        "\n",
        "              if response:\n",
        "                extracted_dates[str(file_name)] = response\n",
        "                print(str(file_name) + ' EXTRACTED!')\n",
        "              else:\n",
        "                print(str(file_name) + ' NOT FOUND!')"
      ],
      "metadata": {
        "id": "Ss5J8bpD_d8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dates extraction with NEW METHOD"
      ],
      "metadata": {
        "id": "jp-4QzmqarZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "insurances_folder_path = \"/content/PDF-Metadata-Extractor/Insurances\"\n",
        "\n",
        "extracted_dates = {}\n",
        "\n",
        "for root, dirs, files in os.walk(insurances_folder_path):\n",
        "    for file_name in tqdm(files):\n",
        "        full_file_path = os.path.join(root, file_name)\n",
        "\n",
        "        # Dates extraction\n",
        "        extractor_dates = PDFDatesFinderSpace(full_file_path)\n",
        "        pages, tables = extractor_dates.extract_mytext()\n",
        "        paragraphs = [extractor_dates.identify_paragraphs_space(page) for page in pages]\n",
        "\n",
        "        # keywords filter 1th level\n",
        "        check_kw = False\n",
        "        for phrase in paragraphs:\n",
        "          for sentence in phrase:\n",
        "            if re.search(r'\\bperiod\\b', sentence, re.IGNORECASE):\n",
        "              check_kw = True\n",
        "\n",
        "        if check_kw:\n",
        "          # Use a list comprehension with regex to keep only the phrases that contain the word \"period\" (case-insensitive)\n",
        "          paragraphs = [sublist for sublist in paragraphs if any(re.search(r'\\bperiod\\b', phrase, re.IGNORECASE) for phrase in sublist)]\n",
        "\n",
        "          # Now, further filter each sublist to keep only the phrases that contain the word \"period\"\n",
        "          paragraphs = [[phrase for phrase in sublist if re.search(r'\\bperiod\\b', phrase, re.IGNORECASE)] for sublist in paragraphs]\n",
        "\n",
        "\n",
        "        responses = []\n",
        "        for phrase in paragraphs:\n",
        "          if len(phrase):\n",
        "            for sentence in phrase:\n",
        "              sentcence = sentence.replace(',', '')\n",
        "              prompt = \"Find start date and end date from the following sentence: \" + sentence\n",
        "              inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "              generate_ids = model.generate(inputs.input_ids, max_length=200)\n",
        "              response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "              response = post_process_response_dates(response)\n",
        "\n",
        "              if response:\n",
        "                  responses.append(response)\n",
        "                  #print(response)\n",
        "\n",
        "        if len(responses)==0:\n",
        "          # Parse with regex for prompt fails\n",
        "          for phrase in paragraphs:\n",
        "            if len(phrase):\n",
        "              if len(phrase)>1:\n",
        "                phrase = [' '.join(phrase)]\n",
        "              for sentence in phrase:\n",
        "                  sentence = sentence.replace(',','')\n",
        "                  response = find_dates_regex(sentence)\n",
        "                  responses.append(response)\n",
        "\n",
        "        if responses:\n",
        "          print(str(file_name) + ' EXTRACTED!')\n",
        "          extracted_dates[str(file_name)] = responses\n",
        "        else:\n",
        "          print(str(file_name) + ' NOT FOUND!')"
      ],
      "metadata": {
        "id": "5qIvItUGavzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save final output in gdrive"
      ],
      "metadata": {
        "id": "dk5Z9E0H1GsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "iLzL3zqS1H2u",
        "outputId": "a02715c1-6fb2-4044-8a33-c6abcb76da36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_dates = \"/content/drive/MyDrive/PDFExtractor /Extracted Dates/extracted_dates.json\"\n",
        "\n",
        "# Scrive il dizionario in un file JSON\n",
        "with open(path_dates, 'w') as file:\n",
        "    json.dump(extracted_dates, file)\n",
        "\n",
        "print(\"File JSON salvato con successo!\")"
      ],
      "metadata": {
        "id": "l-AQbH651T3Z",
        "outputId": "b1149454-3e75-4318-9ef0-2908261272fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File JSON salvato con successo!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Deductibles extraction"
      ],
      "metadata": {
        "id": "k82B5AprFJz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "insurances_folder_path = \"/content/PDF-Metadata-Extractor/Insurances\"\n",
        "\n",
        "# Destination folder in which save deductibles .txt\n",
        "path_deductibles = \"/content/drive/MyDrive/PDFExtractor /Extracted Deductibles\"\n",
        "\n",
        "deductibles = {}\n",
        "\n",
        "for root, dirs, files in os.walk(insurances_folder_path):\n",
        "    for file_name in tqdm(files):\n",
        "        full_file_path = os.path.join(root, file_name)\n",
        "\n",
        "        extractor_deductibles = PDFDeductiblesFinder(full_file_path)\n",
        "        pages, pages_words, tables = extractor_deductibles.extract_mytext()\n",
        "        pages_with_ded = extractor_deductibles.identify_deductibles_pages(pages, pages_words)\n",
        "        deductibles[file_name] = pages_with_ded\n",
        "\n",
        "        file_name_txt = os.path.splitext(file_name)[0] + \".txt\"\n",
        "        file_path_txt = os.path.join(path_deductibles, file_name_txt)\n",
        "        with open(file_path_txt, 'w', encoding='utf-8') as file:\n",
        "            for page in pages_with_ded:\n",
        "                file.write(page)"
      ],
      "metadata": {
        "id": "2xC2eFu6FNFC",
        "outputId": "3ca09a33-d0e5-4bb9-d626-058ed2f88929",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:37<00:00, 37.72s/it]\n"
          ]
        }
      ]
    }
  ]
}