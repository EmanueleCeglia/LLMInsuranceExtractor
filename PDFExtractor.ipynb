{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM1vIKemyCRSImNx1ScVTWf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/EmanueleCeglia/PDF-Metadata-Extractor.git"
      ],
      "metadata": {
        "id": "MHIbcKwxLuKy",
        "outputId": "da1134c8-3316-449f-8a39-060433ebed8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PDF-Metadata-Extractor'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 49 (delta 19), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (49/49), 470.48 KiB | 17.42 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd PDF-Metadata-Extractor"
      ],
      "metadata": {
        "id": "Zf98MyPQL2Lz",
        "outputId": "8f092524-84e2-45e2-ecd5-0a8263c7c419",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PDF-Metadata-Extractor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "U0w-7EwSL9kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install ghostscript -y"
      ],
      "metadata": {
        "id": "MNXmHnKO4x0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Dates"
      ],
      "metadata": {
        "id": "UiWcGaIttFL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from pdf_dates_finder import PDFDatesFinderSemanticSearch  #old method\n",
        "from pdf_dates_finder import PDFDatesFinderSpace   #new method\n",
        "import os\n",
        "import re\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Sl0zsSAz7gHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download model"
      ],
      "metadata": {
        "id": "2zaVl7xYvVfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Open-Orca/OpenOrca-Platypus2-13B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Open-Orca/OpenOrca-Platypus2-13B\", device_map=\"auto\", load_in_8bit=True)"
      ],
      "metadata": {
        "id": "i6qxZyr37hMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def post_process_response_dates(text):\n",
        "    start_date_pattern = r\"Start date:\\s*([0-9]+[a-z]*\\s+[A-Z][a-z]+\\s+[0-9]{4})\"\n",
        "    end_date_pattern = r\"End date:\\s*([0-9]+[a-z]*\\s+[A-Z][a-z]+\\s+[0-9]{4})\"\n",
        "\n",
        "    start_date_match = re.search(start_date_pattern, text)\n",
        "    end_date_match = re.search(end_date_pattern, text)\n",
        "\n",
        "    result = {}\n",
        "    if start_date_match and end_date_match:\n",
        "        result['Start date'] = start_date_match.group(1)\n",
        "        result['End date'] = end_date_match.group(1)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "ihmEWA-uB9Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dates extraction with OLD METHOD"
      ],
      "metadata": {
        "id": "titiBIbjBvUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "insurances_folder_path = \"/content/PDF-Metadata-Extractor/Insurances\"\n",
        "\n",
        "extracted_dates = {}\n",
        "\n",
        "for root, dirs, files in os.walk(insurances_folder_path):\n",
        "    for file_name in tqdm(files):\n",
        "        full_file_path = os.path.join(root, file_name)\n",
        "\n",
        "        extraction = PDFDatesFinderSemanticSearch(full_file_path)\n",
        "        extraction.load_pdf()\n",
        "        extraction.process_text()\n",
        "        dates = extraction.find_dates()\n",
        "\n",
        "        for value in dates.values():\n",
        "          for phrase in value.values():\n",
        "            if len(phrase):\n",
        "              sentence = phrase[0]\n",
        "\n",
        "              prompt = \"Find start date and end date from the following sentence: \" + sentence\n",
        "              inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "              generate_ids = model.generate(inputs.input_ids, max_length=1000)\n",
        "              response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "              response = post_process_response_dates(response)\n",
        "\n",
        "              if response:\n",
        "                extracted_dates[str(file_name)] = response\n",
        "                print(str(file_name) + ' EXTRACTED!')\n",
        "              else:\n",
        "                print(str(file_name) + ' NOT FOUND!')"
      ],
      "metadata": {
        "id": "Ss5J8bpD_d8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dates extraction with NEW METHOD"
      ],
      "metadata": {
        "id": "jp-4QzmqarZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "insurances_folder_path = \"/content/PDF-Metadata-Extractor/Insurances\"\n",
        "\n",
        "extracted_dates = {}\n",
        "\n",
        "for root, dirs, files in os.walk(insurances_folder_path):\n",
        "    for file_name in tqdm(files):\n",
        "        full_file_path = os.path.join(root, file_name)\n",
        "\n",
        "        extractor = PDFDatesFinderSpace(full_file_path)\n",
        "        pages, tables = extractor.extract_mytext()\n",
        "        paragraphs = [extractor.identify_paragraphs_space(page) for page in pages]\n",
        "        #print(paragraphs)\n",
        "\n",
        "        # keywords filter 1th level\n",
        "        check_kw = False\n",
        "        for phrase in paragraphs:\n",
        "          for senence in phrase:\n",
        "            if re.search(r'\\bperiod\\b', senence, re.IGNORECASE):\n",
        "              check_kw = True\n",
        "\n",
        "        if check_kw:\n",
        "          # Use a list comprehension with regex to keep only the phrases that contain the word \"period\" (case-insensitive)\n",
        "          paragraphs = [sublist for sublist in paragraphs if any(re.search(r'\\bperiod\\b', phrase, re.IGNORECASE) for phrase in sublist)]\n",
        "\n",
        "          # Now, further filter each sublist to keep only the phrases that contain the word \"period\"\n",
        "          paragraphs = [[phrase for phrase in sublist if re.search(r'\\bperiod\\b', phrase, re.IGNORECASE)] for sublist in paragraphs]\n",
        "\n",
        "\n",
        "        responses = []\n",
        "        for phrase in paragraphs:\n",
        "          if len(phrase):\n",
        "            for sentence in phrase:\n",
        "              print(sentence)\n",
        "              prompt = \"Find start date and end date from the following sentence: \" + sentence\n",
        "              inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "              generate_ids = model.generate(inputs.input_ids, max_length=200)\n",
        "              response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "              response = post_process_response_dates(response)\n",
        "\n",
        "              if response:\n",
        "                  responses.append(response)\n",
        "                  print(response)\n",
        "\n",
        "        if responses:\n",
        "          print(str(file_name) + ' EXTRACTED!')\n",
        "          extracted_dates[str(file_name)] = responses\n",
        "        else:\n",
        "          print(str(file_name) + ' NOT FOUND!')"
      ],
      "metadata": {
        "id": "5qIvItUGavzo",
        "outputId": "1fa10c24-3873-4150-a619-a279a81fc3e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "policy_20.pdf NOT FOUND!\n",
            "DDRESS: Avenida Presidente Riesco 5711 Of 1201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1468: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": With effect from 30th November 2022 12.01 am, local standard time, at the address of the Insured to 31st May 2024 12.01 am, local standard time, at t\n",
            "{'Start date': '30th November 2022', 'End date': '31st May 2024'}\n",
            "policy_20.pdf EXTRACTED!\n",
            "Weapons Exclusion Clause 10th November, 2003 (Cl.370). Marine Cyber Endorsement LMA5403 as attached. Sanction Limitation and Exclusion Clause JH2010/009 as attached. Coronavirus Exclusion LMA 5395, as attached.\n",
            "itute Fishing Vessel Clauses 20th July, 1987 (Cl.346) eliminating lines 28 to 34 Clause 4 (Termination) – Clause 18 (Liability for Collision) and Clause 20 (Protection and Indemnity). Equipment not attached to the hull, external engine coverage of Full Loss due to the loss of the mother ship only (T.L.V.O.). In respect of Other Vessels: Pursuant to conditions of TLO clauses registered in the registry of insurance policies under POL. 1.92.049, including salvage, salvage expen\n",
            "policy_20.pdf EXTRACTED!\n",
            "ls 1st November, 1995 (Cl.280). SECTION III: Pursuant to additional war and strike clause CAD 1.92.052, excluding terrorism as\n",
            "Weapons Exclusion Clause 10th November, 2003 (Cl.370). Marine Cyber Endorsement LMA5403 as attached. Sanction Limitation and Exclusion Clause JH2010/009 as attached. Exclusion of Electronic Information Clause (CUG 1 02 074). Institute War and Strikes Clauses Hulls – Time 1st November, 1995 (Cl.281). Co\n",
            "policy_20.pdf EXTRACTED!\n",
            "policy_20.pdf EXTRACTED!\n",
            "policy_20.pdf EXTRACTED!\n",
            "policy_20.pdf EXTRACTED!\n",
            "ALFONSO USD 100,000 USD 75,000 DON EDMUNDO USD 90,000 USD 75,000\n",
            "BONN USD 50,000 USD 50,000\n",
            "policy_20.pdf EXTRACTED!\n",
            "policy_20.pdf EXTRACTED!\n",
            "Art. 23 de la POL 1 2013 0694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [05:10<00:00, 310.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "policy_20.pdf EXTRACTED!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_dates"
      ],
      "metadata": {
        "id": "ceoYx4rcl3pS",
        "outputId": "c52790cc-2208-40a3-a69b-c286224d11d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'policy_20.pdf': [{'Start date': '30th November 2022',\n",
              "   'End date': '31st May 2024'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}