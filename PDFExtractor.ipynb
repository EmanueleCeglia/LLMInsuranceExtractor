{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM/gCjZSqUDiw5hS2RL5zf3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/EmanueleCeglia/PDF-Metadata-Extractor.git"
      ],
      "metadata": {
        "id": "MHIbcKwxLuKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd PDF-Metadata-Extractor"
      ],
      "metadata": {
        "id": "Zf98MyPQL2Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "U0w-7EwSL9kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install ghostscript -y"
      ],
      "metadata": {
        "id": "MNXmHnKO4x0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Dates"
      ],
      "metadata": {
        "id": "UiWcGaIttFL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from pdf_dates_finder import PDFDatesFinderSemanticSearch  #old method\n",
        "from pdf_dates_finder import PDFDatesFinderSpace   #new method\n",
        "import os\n",
        "import re\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Sl0zsSAz7gHE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download model"
      ],
      "metadata": {
        "id": "2zaVl7xYvVfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Open-Orca/OpenOrca-Platypus2-13B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Open-Orca/OpenOrca-Platypus2-13B\", device_map=\"auto\", load_in_8bit=True)"
      ],
      "metadata": {
        "id": "i6qxZyr37hMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def post_process_response_dates(text):\n",
        "    start_date_pattern = r\"Start date:\\s*([0-9]+[a-z]*\\s+[A-Z][a-z]+\\s+[0-9]{4})\"\n",
        "    end_date_pattern = r\"End date:\\s*([0-9]+[a-z]*\\s+[A-Z][a-z]+\\s+[0-9]{4})\"\n",
        "\n",
        "    start_date_match = re.search(start_date_pattern, text)\n",
        "    end_date_match = re.search(end_date_pattern, text)\n",
        "\n",
        "    result = {}\n",
        "    if start_date_match and end_date_match:\n",
        "        result['Start date'] = start_date_match.group(1)\n",
        "        result['End date'] = end_date_match.group(1)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "    return result\n",
        "\n",
        "# second level filter\n",
        "def find_dates_regex(string):\n",
        "    pattern = r'\\b\\d{1,2}/\\d{1,2}/\\d{4}\\b|\\b\\d{1,2} [A-Za-z]+ \\d{4}\\b|\\b\\d{1,2}(?:st|nd|rd|th)? [A-Za-z]+ \\d{4}\\b|\\b[A-Za-z]+ \\d{1,2}(?:st|nd|rd|th)? \\d{4}\\b'\n",
        "    dates = re.findall(pattern, string)\n",
        "    if len(dates) >= 2:\n",
        "        return f\"Start date: {dates[0]} End date: {dates[1]}\"\n",
        "    elif len(dates) == 1:\n",
        "        return f\"Start date: {dates[0]} End date: None\"\n",
        "    else:\n",
        "        return \"No dates found\""
      ],
      "metadata": {
        "id": "ihmEWA-uB9Ac"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dates extraction with OLD METHOD"
      ],
      "metadata": {
        "id": "titiBIbjBvUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "insurances_folder_path = \"/content/PDF-Metadata-Extractor/Insurances\"\n",
        "\n",
        "extracted_dates = {}\n",
        "\n",
        "for root, dirs, files in os.walk(insurances_folder_path):\n",
        "    for file_name in tqdm(files):\n",
        "        full_file_path = os.path.join(root, file_name)\n",
        "\n",
        "        extraction = PDFDatesFinderSemanticSearch(full_file_path)\n",
        "        extraction.load_pdf()\n",
        "        extraction.process_text()\n",
        "        dates = extraction.find_dates()\n",
        "\n",
        "        for value in dates.values():\n",
        "          for phrase in value.values():\n",
        "            if len(phrase):\n",
        "              sentence = phrase[0]\n",
        "\n",
        "              prompt = \"Find start date and end date from the following sentence: \" + sentence\n",
        "              inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "              generate_ids = model.generate(inputs.input_ids, max_length=1000)\n",
        "              response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "              response = post_process_response_dates(response)\n",
        "\n",
        "              if response:\n",
        "                extracted_dates[str(file_name)] = response\n",
        "                print(str(file_name) + ' EXTRACTED!')\n",
        "              else:\n",
        "                print(str(file_name) + ' NOT FOUND!')"
      ],
      "metadata": {
        "id": "Ss5J8bpD_d8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dates extraction with NEW METHOD"
      ],
      "metadata": {
        "id": "jp-4QzmqarZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "insurances_folder_path = \"/content/PDF-Metadata-Extractor/Insurances\"\n",
        "\n",
        "extracted_dates = {}\n",
        "\n",
        "for root, dirs, files in os.walk(insurances_folder_path):\n",
        "    for file_name in tqdm(files):\n",
        "        full_file_path = os.path.join(root, file_name)\n",
        "\n",
        "        extractor = PDFDatesFinderSpace(full_file_path)\n",
        "        pages, tables = extractor.extract_mytext()\n",
        "        paragraphs = [extractor.identify_paragraphs_space(page) for page in pages]\n",
        "        #print(paragraphs)\n",
        "\n",
        "        # keywords filter 1th level\n",
        "        check_kw = False\n",
        "        for phrase in paragraphs:\n",
        "          for sentence in phrase:\n",
        "            if re.search(r'\\bperiod\\b', sentence, re.IGNORECASE):\n",
        "              check_kw = True\n",
        "\n",
        "        if check_kw:\n",
        "          # Use a list comprehension with regex to keep only the phrases that contain the word \"period\" (case-insensitive)\n",
        "          paragraphs = [sublist for sublist in paragraphs if any(re.search(r'\\bperiod\\b', phrase, re.IGNORECASE) for phrase in sublist)]\n",
        "\n",
        "          # Now, further filter each sublist to keep only the phrases that contain the word \"period\"\n",
        "          paragraphs = [[phrase for phrase in sublist if re.search(r'\\bperiod\\b', phrase, re.IGNORECASE)] for sublist in paragraphs]\n",
        "\n",
        "\n",
        "        responses = []\n",
        "        for phrase in paragraphs:\n",
        "          if len(phrase):\n",
        "            for sentence in phrase:\n",
        "              sentcence = sentence.replace(',', '')\n",
        "              prompt = \"Find start date and end date from the following sentence: \" + sentence\n",
        "              inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "              generate_ids = model.generate(inputs.input_ids, max_length=200)\n",
        "              response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "              response = post_process_response_dates(response)\n",
        "\n",
        "              if response:\n",
        "                  responses.append(response)\n",
        "                  print(response)\n",
        "\n",
        "        if responses:\n",
        "          print(str(file_name) + ' EXTRACTED!')\n",
        "          extracted_dates[str(file_name)] = responses\n",
        "        else:\n",
        "          # Parse with regex\n",
        "          for phrase in paragraphs:\n",
        "            if len(phrase):\n",
        "              if len(phrase)>1:\n",
        "                phrase = [' '.join(phrase)]\n",
        "              for sentence in phrase:\n",
        "                  sentence = sentence.replace(',','')\n",
        "                  response = find_dates_regex(sentence)\n",
        "                  responses.append(response)\n",
        "        if responses:\n",
        "          print(str(file_name) + ' EXTRACTED!')\n",
        "          extracted_dates[str(file_name)] = responses\n",
        "        else:\n",
        "          print(str(file_name) + ' NOT FOUND!')"
      ],
      "metadata": {
        "id": "5qIvItUGavzo",
        "outputId": "7e732da2-4009-4d49-e0fd-166f2bbef216",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1468: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "100%|██████████| 1/1 [01:01<00:00, 61.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "policy_39.pdf EXTRACTED!\n",
            "policy_39.pdf NOT FOUND!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_dates"
      ],
      "metadata": {
        "id": "ceoYx4rcl3pS",
        "outputId": "8fd2f4df-827a-4973-d710-7b47eebb8d6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'policy_39.pdf': ['Start date: 31st March 2022 End date: 31st March 2023']}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}