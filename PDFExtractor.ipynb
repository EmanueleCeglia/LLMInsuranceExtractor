{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOkUSTohV31UzqP38mjTdd9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/EmanueleCeglia/PDF-Metadata-Extractor.git"
      ],
      "metadata": {
        "id": "MHIbcKwxLuKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd PDF-Metadata-Extractor"
      ],
      "metadata": {
        "id": "Zf98MyPQL2Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "U0w-7EwSL9kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install ghostscript -y"
      ],
      "metadata": {
        "id": "MNXmHnKO4x0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Dates"
      ],
      "metadata": {
        "id": "UiWcGaIttFL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from pdf_extractor import PDFDatesFinderSemanticSearch  #old method\n",
        "from pdf_extractor import PDFDatesFinderSpace   #new method\n",
        "from pdf_extractor import PDFDeductiblesFinder\n",
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import json\n",
        "import openai"
      ],
      "metadata": {
        "id": "Sl0zsSAz7gHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "ecbGH6JjaenZ",
        "outputId": "5a795078-ca6c-49d1-dabc-2b8143d9798c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Download models"
      ],
      "metadata": {
        "id": "2zaVl7xYvVfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenOrca-Platyous2-13B works well for dates extractions"
      ],
      "metadata": {
        "id": "P-akpja9kjN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Open-Orca/OpenOrca-Platypus2-13B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Open-Orca/OpenOrca-Platypus2-13B\", device_map=\"auto\", load_in_8bit=True)"
      ],
      "metadata": {
        "id": "i6qxZyr37hMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def post_process_response_dates(text):\n",
        "    start_date_pattern = r\"Start date:\\s*([0-9]+[a-z]*\\s+[A-Z][a-z]+\\s+[0-9]{4})\"\n",
        "    end_date_pattern = r\"End date:\\s*([0-9]+[a-z]*\\s+[A-Z][a-z]+\\s+[0-9]{4})\"\n",
        "\n",
        "    start_date_match = re.search(start_date_pattern, text)\n",
        "    end_date_match = re.search(end_date_pattern, text)\n",
        "\n",
        "    result = {}\n",
        "    if start_date_match and end_date_match:\n",
        "        result['Start date'] = start_date_match.group(1)\n",
        "        result['End date'] = end_date_match.group(1)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "    return result\n",
        "\n",
        "# second level filter\n",
        "def find_dates_regex(string):\n",
        "    pattern = r'\\b\\d{1,2}/\\d{1,2}/\\d{4}\\b|\\b\\d{1,2} [A-Za-z]+ \\d{4}\\b|\\b\\d{1,2}(?:st|nd|rd|th)? [A-Za-z]+ \\d{4}\\b|\\b[A-Za-z]+ \\d{1,2}(?:st|nd|rd|th)? \\d{4}\\b'\n",
        "    dates = re.findall(pattern, string)\n",
        "    if len(dates) >= 2:\n",
        "        return f\"Start date: {dates[0]} End date: {dates[1]}\"\n",
        "    elif len(dates) == 1:\n",
        "        return f\"Start date: {dates[0]} End date: None\"\n",
        "    else:\n",
        "        return \"No dates found\""
      ],
      "metadata": {
        "id": "ihmEWA-uB9Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dates extraction with OLD METHOD (don't use)"
      ],
      "metadata": {
        "id": "titiBIbjBvUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# insurances_folder_path = \"/content/PDF-Metadata-Extractor/Insurances\"\n",
        "\n",
        "# extracted_dates = {}\n",
        "\n",
        "# for root, dirs, files in os.walk(insurances_folder_path):\n",
        "#     for file_name in tqdm(files):\n",
        "#         full_file_path = os.path.join(root, file_name)\n",
        "\n",
        "#         extraction = PDFDatesFinderSemanticSearch(full_file_path)\n",
        "#         extraction.load_pdf()\n",
        "#         extraction.process_text()\n",
        "#         dates = extraction.find_dates()\n",
        "\n",
        "#         for value in dates.values():\n",
        "#           for phrase in value.values():\n",
        "#             if len(phrase):\n",
        "#               sentence = phrase[0]\n",
        "\n",
        "#               prompt = \"Find start date and end date from the following sentence: \" + sentence\n",
        "#               inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "#               generate_ids = model.generate(inputs.input_ids, max_length=1000)\n",
        "#               response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "#               response = post_process_response_dates(response)\n",
        "\n",
        "#               if response:\n",
        "#                 extracted_dates[str(file_name)] = response\n",
        "#                 print(str(file_name) + ' EXTRACTED!')\n",
        "#               else:\n",
        "#                 print(str(file_name) + ' NOT FOUND!')"
      ],
      "metadata": {
        "id": "Ss5J8bpD_d8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dates extraction with NEW METHOD"
      ],
      "metadata": {
        "id": "jp-4QzmqarZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "insurances_folder_path = \"/content/PDF-Metadata-Extractor/Insurances\"\n",
        "\n",
        "extracted_dates = {}\n",
        "\n",
        "for root, dirs, files in os.walk(insurances_folder_path):\n",
        "    for file_name in tqdm(files):\n",
        "        full_file_path = os.path.join(root, file_name)\n",
        "\n",
        "        # Dates extraction\n",
        "        extractor_dates = PDFDatesFinderSpace(full_file_path)\n",
        "        pages, tables = extractor_dates.extract_mytext()\n",
        "        paragraphs = [extractor_dates.identify_paragraphs_space(page) for page in pages]\n",
        "\n",
        "        # keywords filter 1th level\n",
        "        check_kw = False\n",
        "        for phrase in paragraphs:\n",
        "          for sentence in phrase:\n",
        "            if re.search(r'\\bperiod\\b', sentence, re.IGNORECASE):\n",
        "              check_kw = True\n",
        "\n",
        "        if check_kw:\n",
        "          # Use a list comprehension with regex to keep only the phrases that contain the word \"period\" (case-insensitive)\n",
        "          paragraphs = [sublist for sublist in paragraphs if any(re.search(r'\\bperiod\\b', phrase, re.IGNORECASE) for phrase in sublist)]\n",
        "\n",
        "          # Now, further filter each sublist to keep only the phrases that contain the word \"period\"\n",
        "          paragraphs = [[phrase for phrase in sublist if re.search(r'\\bperiod\\b', phrase, re.IGNORECASE)] for sublist in paragraphs]\n",
        "\n",
        "\n",
        "        responses = []\n",
        "        for phrase in paragraphs:\n",
        "          if len(phrase):\n",
        "            for sentence in phrase:\n",
        "              sentcence = sentence.replace(',', '')\n",
        "              prompt = \"Find start date and end date from the following sentence: \" + sentence\n",
        "              inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "              generate_ids = model.generate(inputs.input_ids, max_length=200)\n",
        "              response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "              response = post_process_response_dates(response)\n",
        "\n",
        "              if response:\n",
        "                  responses.append(response)\n",
        "                  #print(response)\n",
        "\n",
        "        if len(responses)==0:\n",
        "          # Parse with regex for prompt fails\n",
        "          for phrase in paragraphs:\n",
        "            if len(phrase):\n",
        "              if len(phrase)>1:\n",
        "                phrase = [' '.join(phrase)]\n",
        "              for sentence in phrase:\n",
        "                  sentence = sentence.replace(',','')\n",
        "                  response = find_dates_regex(sentence)\n",
        "                  responses.append(response)\n",
        "\n",
        "        if responses:\n",
        "          print(str(file_name) + ' EXTRACTED!')\n",
        "          extracted_dates[str(file_name)+\"_dates\"] = responses\n",
        "        else:\n",
        "          print(str(file_name) + ' NOT FOUND!')"
      ],
      "metadata": {
        "id": "5qIvItUGavzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Deductibles extraction using GPT-3.5 turbo"
      ],
      "metadata": {
        "id": "k82B5AprFJz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ChatGPT_conversation(conversation):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model_id,\n",
        "        messages=conversation\n",
        "    )\n",
        "    conversation.append({'role': response.choices[0].message.role, 'content': response.choices[0].message.content})\n",
        "    return conversation"
      ],
      "metadata": {
        "id": "ZkZgap4LMwmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insurances_folder_path = \"/content/PDF-Metadata-Extractor/Insurances\"\n",
        "\n",
        "deductibles = {}\n",
        "extracted_deductibles = {}\n",
        "\n",
        "API_KEY = \"xxx\"\n",
        "openai.api_key = API_KEY\n",
        "model_id = 'gpt-3.5-turbo'\n",
        "\n",
        "for root, dirs, files in os.walk(insurances_folder_path):\n",
        "    for file_name in tqdm(files):\n",
        "        conversation = []\n",
        "        full_file_path = os.path.join(root, file_name)\n",
        "\n",
        "        extractor_deductibles = PDFDeductiblesFinder(full_file_path)\n",
        "        pages, pages_words, tables = extractor_deductibles.extract_mytext()\n",
        "        pages_with_ded = extractor_deductibles.identify_deductibles_pages(pages, pages_words)\n",
        "        deductibles[file_name] = pages_with_ded\n",
        "\n",
        "        prompt = 'User:' + \" Extract only information about DEDUCTIBLES from the following text: \" + deductibles[file_name]\n",
        "        conversation.append({'role': 'user', 'content': prompt})\n",
        "        conversation = ChatGPT_conversation(conversation)\n",
        "        extracted_deductibles[file_name + \"_deductibles\"] = [{'Deductible': conversation[-1]['content'].strip()}]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2xC2eFu6FNFC",
        "outputId": "a0299044-6b86-44d9-fc6d-048f5f66b016",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:46<00:00, 46.17s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save output as .json file"
      ],
      "metadata": {
        "id": "9Z9jbLIMOG3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Merge(dict_1, dict_2):\n",
        "\tresult = dict_1 | dict_2\n",
        "\treturn result\n",
        "\n",
        "final_dict = Merge(extracted_dates, extracted_deductibles)\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/PDFExtractor /Extractions/extractions.json\"\n",
        "\n",
        "# Scrive il dizionario in un file JSON\n",
        "with open(output_path, 'w') as file:\n",
        "    json.dump(final_dict, file)\n",
        "\n",
        "print(\"File JSON salvato con successo!\")"
      ],
      "metadata": {
        "id": "67vdvAKYQDKR",
        "outputId": "382c20ea-af17-4280-acb9-528d08e318d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File JSON salvato con successo!\n"
          ]
        }
      ]
    }
  ]
}